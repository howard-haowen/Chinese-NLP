{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gensim playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP77g7lR92CYS7CoDP+mjO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howard-haowen/NLP-demos/blob/main/gensim_playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqDpf9n4GUaq"
      },
      "outputs": [],
      "source": [
        "from traitlets.config.manager import BaseJSONConfigManager\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
        "cm = BaseJSONConfigManager(config_dir=str(path))\n",
        "cm.update(\n",
        "    \"rise\",\n",
        "    {\n",
        "        \"autolaunch\": False,\n",
        "        \"enable_chalkboard\": True,\n",
        "        \"scroll\": True,\n",
        "        \"slideNumber\": True,\n",
        "        \"controls\": True,\n",
        "        \"progress\": True,\n",
        "        \"history\": True,\n",
        "        \"center\": True,\n",
        "        \"width\": \"100%\",\n",
        "        \"height\": \"100%\",\n",
        "        \"theme\": \"beige\",\n",
        "        \"transition\": \"concave\",\n",
        "        \"start_slideshow_at\": \"selected\"\n",
        "     }\n",
        ")"
      ],
      "metadata": {
        "id": "8u6jpKZEGfqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "UZyGm7rIGmsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install pyldavis\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "ldix_r78GgS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "import spacy\n",
        "# corpus\n",
        "import nltk\n",
        "# topic modelling\n",
        "import gensim\n",
        "from gensim.models import LdaModel, HdpModel\n",
        "from gensim.corpora import Dictionary\n",
        "# visualization\n",
        "import pyLDAvis.gensim_models\n",
        "# tabularization of data\n",
        "import pandas as pd\n",
        "# others\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now"
      ],
      "metadata": {
        "id": "DxvFLyAzGwPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the corpus"
      ],
      "metadata": {
        "id": "473O8H9eG6aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews"
      ],
      "metadata": {
        "id": "FlLhyTrHG6wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_ids = movie_reviews.fileids()\n",
        "len(file_ids)"
      ],
      "metadata": {
        "id": "ETWDI5ulHKEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews.raw(file_ids[0])"
      ],
      "metadata": {
        "id": "vXVqbMTeHLFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def moviews2df(file_ids):\n",
        "    sentiments = [1 if id[:3] == 'pos' else 0 for id in file_ids]\n",
        "    texts = [movie_reviews.raw(id) for id in file_ids]\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"sentiment\": sentiments,\n",
        "            \"text\": texts,\n",
        "        }\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "CtFDj9d0HMqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = moviews2df(file_ids)\n",
        "df"
      ],
      "metadata": {
        "id": "3Trg2i7dHN_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "C7KOvv3nHXaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hnty9gsmHXxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_texts(texts):\n",
        "    clean_texts = []\n",
        "    for text in texts:\n",
        "        clean_text = [char if char.isascii() else \" \" for char in text]\n",
        "        clean_text = \"\".join(clean_text)\n",
        "        clean_texts.append(clean_text)\n",
        "    \n",
        "    # nlp.pipe() is more efficient than nlp()\n",
        "    clean_tokens = []\n",
        "    for doc in nlp.pipe(clean_texts, disable=[\"ner\", \"tagger\", \"parser\"]):\n",
        "        tokens = [tok.lemma_.lower() for tok in doc if (not tok.is_stop) and (not tok.is_punct) and (not tok.like_num) and (not tok.is_space)]\n",
        "        clean_tokens.append(tokens)\n",
        "\n",
        "    return clean_tokens"
      ],
      "metadata": {
        "id": "CoCejDHnHhjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens'] = preprocess_texts(df['text'])\n",
        "df"
      ],
      "metadata": {
        "id": "yL8AKwPcHiJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modelling"
      ],
      "metadata": {
        "id": "4mgkazs9H4GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus = \n",
        "# dictionary = \n",
        "# model = "
      ],
      "metadata": {
        "id": "XMUuyJfNH9-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "acQukv0vIOsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "GyRPYPGGIQ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "uujsiuAwIgH_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}